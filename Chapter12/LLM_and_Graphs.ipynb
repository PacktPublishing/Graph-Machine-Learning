{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMM and Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aH3Q56MGDARI"
   },
   "source": [
    "Let's create a toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lDLHAhIlDEvm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/euler/.conda/envs/chap12/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Assume a toy dataset with 3 papers (nodes), edges, and labels\n",
    "data = Data(\n",
    "    x=torch.rand(3, 10),  # Random node features\n",
    "    edge_index=torch.tensor([[0, 1], [1, 2]], dtype=torch.long).t().contiguous(),  # Edges (transposed for PyG)\n",
    "    y=torch.tensor([0, 1, 2], dtype=torch.long),  # True labels (3 classes)\n",
    "    text=[\"Paper A abstract about machine learning\", \n",
    "          \"Paper B abstract about deep learning\", \n",
    "          \"Paper C abstract about neural networks\"],  # Text data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info:\n",
      "  Number of nodes: 3\n",
      "  Node feature dimension: 10\n",
      "  Number of edges: 2\n",
      "  Number of classes: 3\n",
      "  True labels: [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(torch.unique(data.y))  # Number of unique classes\n",
    "\n",
    "print(f\"Dataset info:\")\n",
    "print(f\"  Number of nodes: {data.x.size(0)}\")\n",
    "print(f\"  Node feature dimension: {data.x.size(1)}\")\n",
    "print(f\"  Number of edges: {data.edge_index.size(1)}\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n",
    "print(f\"  True labels: {data.y.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388,
     "referenced_widgets": [
      "a0f9087c1517444d9d02d3ee239871c0",
      "49409686002944ba913043f9e566dec6",
      "9ca2c22d54a14543a0ccb418f84dfc7a",
      "6ef1fa9fc050493a88aa2ba634c36cb4",
      "95dde357b3704448bfdb0b3a8818413e",
      "918e3e0abaaa43119b27a725d9996622",
      "c45bf94c8ae1497bb5093dd459e35868",
      "78a75c6a1b0444baa32a8dc0444c6e42",
      "efae36a6383c472c8d8fe7f39223a13f",
      "722fd967e7a640468b2c1c828c6ad983",
      "9b6d7d228f25418aaf2489fedc465492",
      "0b92a27e384b4ccfa1717e2898c44499",
      "694e7d436acc4f5288c6dddcb1e8eb43",
      "f8dd5aa75d5342b9be4b9ef0914b3330",
      "577d03563d0e429da232b47df6dc2cfb",
      "da4c66ea361b4ea4be05010c02866f51",
      "f500df21edf1499c83e6d0036e5b771c",
      "26329100e08f452bba6a2cb0199c84b7",
      "bcbc986b382a4ba49bfc4d5fd428488c",
      "bced6297dc0b46ea826ee40344c21b13",
      "04e572cf4bfe4c4b91b08f47cdb88c77",
      "1c795237b0964f4182a2f2948b747f94",
      "63bc10c1908945928df1d9b492912a04",
      "a87bb9d072ee43a08a97c9effb82db9c",
      "000c7c5876a04721ac8ccc89d9e6163e",
      "ccbc8df9abab48948c5317bcc31612e1",
      "87df42e6746e4847864eda694620750f",
      "a88f61637e6541f589187d5d9abcb503",
      "bb0d43a5c4f34b1d9a39230b9c129fd5",
      "5bf85ad0b72a4e68914487ffabef5ea6",
      "f8d2202f01874c30a3ae563e6fdd6428",
      "853f6e10e86f4d8aba5822790d43a34e",
      "581e482879434621bb48245bc886baa2",
      "b369bdad965b4b3794af271f5416d42a",
      "07c929ea8b0e4f6a85bfa54b60beec27",
      "e2e8b3e0ceb7405fa6c0940730f60804",
      "0c381c9ceeb04677903611735c1b7fcf",
      "90fc7f82fa7c4fe79386e766fe687370",
      "02035c4cf9f142c6a7167d4d81befce0",
      "0ef9969aa23348e39e56981b0ce9ba35",
      "ba9d04c9c86e45f9a80f3a61ee220c5e",
      "317a758830264800bbd6f17ee10d5e22",
      "a54841dde9cd42db85b326faaf4ab0b5",
      "72c8076ed09748e1a37b21f25d12dafe",
      "b78234a537214afebcfdee5f32d4d9a0",
      "4090d3b6cdbf45e9b54f65e55a994713",
      "41bb03671ae14450aaeb13b9ee9a9ffa",
      "0825bfd7305b497abaac3d5edbcfb72d",
      "03c8fbe4a1ab403dae44d77ff0240d9a",
      "204a396320264c7fa63b2e539f1b25d1",
      "6a2f31e069e54cfb96dfbb5b80e5d15b",
      "45dc666b5ac94bbf9164058d0c7cd3e2",
      "07718c8acd3049c4a38778e6f8d4eca3",
      "e053ba80dc79470d8b1d1c9dc67db792",
      "bab624c7fda8476d974253c9fba8a827"
     ]
    },
    "id": "YxK0emCj_0ad",
    "outputId": "f823051a-2dab-49df-b472-5373cde384a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Iteration 1: GNN Loss = 1.1190, LLM Loss = 0.9824\n",
      "  GNN predictions: [1, 1, 1]\n",
      "  LLM predictions: [1, 1, 1]\n",
      "Iteration 2: GNN Loss = 0.8503, LLM Loss = 1.0020\n",
      "  GNN predictions: [1, 1, 1]\n",
      "  LLM predictions: [2, 2, 1]\n",
      "Iteration 3: GNN Loss = 0.9946, LLM Loss = 0.9380\n",
      "  GNN predictions: [1, 1, 1]\n",
      "  LLM predictions: [1, 2, 1]\n",
      "Iteration 4: GNN Loss = 0.8984, LLM Loss = 0.9884\n",
      "  GNN predictions: [1, 1, 1]\n",
      "  LLM predictions: [0, 1, 1]\n",
      "Iteration 5: GNN Loss = 0.9916, LLM Loss = 0.7875\n",
      "  GNN predictions: [1, 1, 1]\n",
      "  LLM predictions: [1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. Define the Graph Neural Network (GNN)\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, num_classes)  # Output num_classes\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x  # Return logits (not softmax)\n",
    "\n",
    "# 2. Define the Text Encoder (BERT-based)\n",
    "class TextEncoder(torch.nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", num_classes=3):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        # Project from BERT's hidden size to number of classes\n",
    "        self.classifier = torch.nn.Linear(self.model.config.hidden_size, num_classes)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, texts):\n",
    "        # Tokenize and encode text data\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():  # Freeze BERT parameters during training\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        # Use [CLS] token embedding\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_embedding = self.dropout(cls_embedding)\n",
    "        logits = self.classifier(cls_embedding)\n",
    "        return logits  # Return logits (not softmax)\n",
    "\n",
    "\n",
    "# 4. Training Loop with Bidirectional Pseudo-label Exchange\n",
    "def train_prediction_alignment(data, gnn, text_encoder, num_iterations=5):\n",
    "    optimizer_gnn = torch.optim.Adam(gnn.parameters(), lr=0.01)\n",
    "    optimizer_text = torch.optim.Adam(text_encoder.parameters(), lr=0.0001)\n",
    "    \n",
    "    # Initialize with true labels for first iteration\n",
    "    gnn_pseudo_labels = data.y.clone()\n",
    "    llm_pseudo_labels = data.y.clone()\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # 4.1 Train GNN using LLM pseudo-labels from previous iteration\n",
    "        gnn.train()\n",
    "        optimizer_gnn.zero_grad()\n",
    "        gnn_logits = gnn(data.x, data.edge_index)\n",
    "        gnn_loss = torch.nn.CrossEntropyLoss()(gnn_logits, llm_pseudo_labels)\n",
    "        gnn_loss.backward()\n",
    "        optimizer_gnn.step()\n",
    "        \n",
    "        # Generate new GNN pseudo-labels\n",
    "        with torch.no_grad():\n",
    "            gnn_pseudo_labels = torch.argmax(gnn_logits, dim=1)\n",
    "        \n",
    "        # 4.2 Train Text Encoder using GNN pseudo-labels\n",
    "        text_encoder.train()\n",
    "        optimizer_text.zero_grad()\n",
    "        text_logits = text_encoder(data.text)\n",
    "        llm_loss = torch.nn.CrossEntropyLoss()(text_logits, gnn_pseudo_labels)\n",
    "        llm_loss.backward()\n",
    "        optimizer_text.step()\n",
    "        \n",
    "        # Generate new LLM pseudo-labels for next iteration\n",
    "        with torch.no_grad():\n",
    "            llm_pseudo_labels = torch.argmax(text_logits, dim=1)\n",
    "        \n",
    "        print(f\"Iteration {iteration+1}: GNN Loss = {gnn_loss.item():.4f}, LLM Loss = {llm_loss.item():.4f}\")\n",
    "        print(f\"  GNN predictions: {gnn_pseudo_labels.tolist()}\")\n",
    "        print(f\"  LLM predictions: {llm_pseudo_labels.tolist()}\")\n",
    "\n",
    "# Initialize models and train\n",
    "input_dim = data.x.size(1)  # Node feature dimension\n",
    "hidden_dim = 64\n",
    "\n",
    "gnn = GNN(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes)\n",
    "text_encoder = TextEncoder(num_classes=num_classes)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "train_prediction_alignment(data, gnn, text_encoder, num_iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AwmGzCmH_669",
    "outputId": "2577a628-8f1c-4efb-c732-1dfbbade06bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.1974958181381226\n",
      "Epoch 2: Loss = 0.9338013529777527\n",
      "Epoch 3: Loss = 1.0983785390853882\n",
      "Epoch 4: Loss = 1.0986661911010742\n",
      "Epoch 5: Loss = 1.0841368436813354\n",
      "Epoch 6: Loss = 1.0974050760269165\n",
      "Epoch 7: Loss = 0.621391773223877\n",
      "Epoch 8: Loss = 0.5264388918876648\n",
      "Epoch 9: Loss = 1.1055184602737427\n",
      "Epoch 10: Loss = 1.0985291004180908\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# 1. Define the GNN\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv = GraphConv(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.conv(x, edge_index)\n",
    "\n",
    "# 2. Define the Text Encoder (LLM)\n",
    "class TextEncoder(torch.nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", output_dim=128):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.fc = torch.nn.Linear(self.model.config.hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, texts):\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = self.model(**inputs)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token embedding\n",
    "        return self.fc(cls_embedding)\n",
    "\n",
    "# 3. Contrastive Learning Objective\n",
    "def contrastive_loss(graph_emb, text_emb, tau=0.1):\n",
    "    sim = F.cosine_similarity(graph_emb.unsqueeze(1), text_emb.unsqueeze(0), dim=2)\n",
    "    labels = torch.arange(sim.size(0)).to(sim.device)\n",
    "    loss = F.cross_entropy(sim / tau, labels)\n",
    "    return loss\n",
    "\n",
    "# 4. Training Loop for Latent Space Alignment\n",
    "def train_latent_alignment(data, gnn, text_encoder, epochs=10):\n",
    "    optimizer = torch.optim.Adam(list(gnn.parameters()) + list(text_encoder.parameters()), lr=0.001)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Encode graph and text\n",
    "        graph_emb = gnn(data.x, data.edge_index)  # Graph embeddings\n",
    "        text_emb = text_encoder(data.text)  # Text embeddings\n",
    "\n",
    "        # Compute contrastive loss\n",
    "        loss = contrastive_loss(graph_emb, text_emb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss = {loss.item()}\")\n",
    "\n",
    "# 5. Example Data\n",
    "# Toy data with 3 products and their relationships\n",
    "data = Data(\n",
    "    x=torch.rand(3, 10),  # Node features\n",
    "    edge_index=torch.tensor([[0, 1], [1, 2]], dtype=torch.long),  # Edges\n",
    "    text=[\"Product A description\", \"Product B description\", \"Product C description\"],  # Text data\n",
    ")\n",
    "\n",
    "# Initialize models and train\n",
    "gnn = GNN(input_dim=10, hidden_dim=128)\n",
    "text_encoder = TextEncoder()\n",
    "train_latent_alignment(data, gnn, text_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqcZZn6qCCmj"
   },
   "source": [
    "# GraphRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xM73g-bDiPPG"
   },
   "source": [
    "If using Colab you can simply run the following cells.\n",
    "\n",
    "Otherwise, if you want to use the local backend, please:\n",
    "- download neo4j desktop on [docker](https://neo4j.com/docs/graph-data-science/current/installation/installation-docker/)*\n",
    "- download [lm-studio](https://lmstudio.ai/) and download the minicpm-llama3-v-2_5 and nomic-embed-text model\n",
    "\n",
    "*run docker as:\n",
    "\n",
    "\n",
    "```\n",
    "docker run --rm --env NEO4J_AUTH=neo4j/defaultpass -p 7474:7474 -p 7687:7687 -v $PWD/data:/data -v $PWD/plugins:/plugins --name neo4j-apoc -e NEO4J_apoc_export_file_enabled=true -e NEO4J_apoc_import_file_enabled=true -e NEO4J_apoc_import_file_use__neo4j__config=true -e NEO4J_PLUGINS=\\[\\\"apoc-extended\\\"\\] neo4j\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JKEORJfWwI7w"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "LLM_BACKEND = \"ollama\" # choose [\"ollama\" | \"lm-studio\"]\n",
    "# LLM_BACKEND = \"lm-studio\"\n",
    "\n",
    "assert LLM_BACKEND in [\"ollama\", \"lm-studio\"]\n",
    "\n",
    "if LLM_BACKEND == \"ollama\":\n",
    "  base_url = f\"http://{os.environ.get('OLLAMA_HOST', 'localhost')}:11434/v1\"\n",
    "  api_key = \"ollama\"\n",
    "  # llm_model = \"minicpm-v\"\n",
    "  llm_model = \"phi4\"\n",
    "else:\n",
    "  base_url = \"http://localhost:1234/v1\"\n",
    "  api_key = \"lm-studio\"\n",
    "  llm_model = \"minicpm-llama3-v-2_5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIEW2lBMqp2V"
   },
   "source": [
    "If Colab you need to download ollama and start the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# ollama.pull(llm_model)\n",
    "# ollama.pull(\"nomic-embed-text\")\n",
    "ollama.pull(\"phi4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[Model(model='phi4:latest', modified_at=datetime.datetime(2025, 6, 21, 20, 30, 17, 738327, tzinfo=TzInfo(UTC)), digest='ac896e5b8b34a1f4efa7b14d7520725140d5512484457fab45d2a4ea14c69dba', size=9053116391, details=ModelDetails(parent_model='', format='gguf', family='phi3', families=['phi3'], parameter_size='14.7B', quantization_level='Q4_K_M'))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bK17PDF7KxCv"
   },
   "source": [
    "# Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUWxS0p0-d-e",
    "outputId": "527d26fc-0dfc-479b-f1c8-330ec8b7d52b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "\n",
    "host = os.environ.get(\"NEO4J_HOST\", \"localhost\")\n",
    "\n",
    "# ---- Step 1: Setup Neo4j Connection ----\n",
    "NEO4J_URI = f\"bolt://{host}:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"neo5j\"\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "graph = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USER, password=NEO4J_PASSWORD)\n",
    "\n",
    "# ---- Step 2: Create knowledge graph from text ----\n",
    "import os\n",
    "from langchain_experimental.graph_transformers.llm import LLMGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0,\n",
    "                 model_name=llm_model,\n",
    "                 base_url=base_url,\n",
    "                 api_key=api_key)\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "text = \"\"\"\n",
    "Marie Curie, born in 1867, was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity.\n",
    "She was the first woman to win a Nobel Prize, the first person to win a Nobel Prize twice, and the only person to win a Nobel Prize in two scientific fields.\n",
    "Her husband, Pierre Curie, was a co-winner of her first Nobel Prize, making them the first-ever married couple to win the Nobel Prize and launching the Curie family legacy of five Nobel Prizes.\n",
    "She was, in 1906, the first woman to become a professor at the University of Paris.\n",
    "\"\"\"\n",
    "documents = [Document(page_content=text)]\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "print(f\"Nodes:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships:{graph_documents[0].relationships}\")\n",
    "\n",
    "# Add graph to neo4j\n",
    "graph.add_graph_documents(graph_documents)\n",
    "\n",
    "# ---- Step 3: Perform GraphRAG ----\n",
    "\n",
    "def escape(s):\n",
    "  return s.replace(\"{\",\"\").replace(\"}\",\"\")\n",
    "\n",
    "CYPHER_GENERATION_TEMPLATE = f\"\"\"You are a Neo4j expert. Generate a Cypher query to answer the given question.\n",
    "\n",
    "Database Schema: {escape(graph.schema)}\n",
    "\n",
    "Rules:\n",
    "1. Always use explicit `MATCH` for relationships.\n",
    "2. Never use `WHERE` for relationship matching.\n",
    "3. Use `RETURN DISTINCT` when appropriate.\n",
    "\n",
    "Example Queries:\n",
    "1. Question: \"Who won the Nobel Prize?\"\n",
    "   Cypher: MATCH (p:Person)-[:WON_NOBEL_PRIZE]->(:Awarded) RETURN p.id AS winner\n",
    "\n",
    "Question: {{query}}\n",
    "Return only the Cypher query without any explanation or additional text.\n",
    "Cypher:\"\"\"\n",
    "\n",
    "from langchain_neo4j import GraphCypherQAChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    cypher_prompt=PromptTemplate(\n",
    "        input_variables=[\"query\"],\n",
    "        template=CYPHER_GENERATION_TEMPLATE\n",
    "    ),\n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "# ---- Step 5: Test Queries ----\n",
    "print(\"\\nTesting queries...\")\n",
    "\n",
    "question = \"Who married a Nobel Prize?\"\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "response = chain.invoke(question)\n",
    "print(\"Response:\", response['result'])\n",
    "\n",
    "# Close the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4gzGQzeeqFC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "chap12",
   "language": "python",
   "name": "chap12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
