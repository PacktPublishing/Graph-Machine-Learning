{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9LbvNo43_S8s"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# install PyTorch Geometric if running on Google Colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "  !pip install node2vec\n",
    "  import torch\n",
    "\n",
    "  def format_pytorch_version(version):\n",
    "    return version.split('+')[0]\n",
    "\n",
    "  TORCH_version = torch.__version__\n",
    "  TORCH = format_pytorch_version(TORCH_version)\n",
    "\n",
    "  def format_cuda_version(version):\n",
    "    return 'cu' + version.replace('.', '')\n",
    "\n",
    "  CUDA_version = torch.version.cuda\n",
    "  CUDA = format_cuda_version(CUDA_version)\n",
    "\n",
    "  !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "  !pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "  !pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "  !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "  !pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIK1pgU4_lWd"
   },
   "source": [
    "## Graph embedding-based methods\n",
    "\n",
    "Such techniques seek to apply graph embedding techniques to obtain node-level or graph-level representations and further use the representations for similarity learning. For example, DeepWalk and Node2Vec can be used to extract meaningful embedding that can then be used to define a similarity function or to predict similarity scores. For example, in Tixier et al. (2015), node2vec was used for encoding node embeddings for representing a graph as an image. Specifically, two-dimensional (2D) histograms obtained from those node embeddings were passed to a classical 2D convolutional neural network (CNN) architecture designed for images. Such a simple yet powerful approach enabled good results to be obtained for many benchmark datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701,
     "referenced_widgets": [
      "947a2ce0171c4582843c5afb590471fb",
      "f43faf84d7f041b08bcbec1aeeb5297a",
      "d3fda1a8024c4f308dc36d2216006a46",
      "ebd67e8c91f94cda8c314ff8c387a7d4",
      "4a1994565cd541389bacb618bfb889cc",
      "9b6d7a51282e46409eedb05fa786e629",
      "0a0a68b09ba54987b47fd441d23c8870",
      "28ce9bcc08504d7eb458d15b955005a9",
      "a17cd8706d5a4961858297899f996a86",
      "93995324744e414b8b88453ea975245f",
      "66779461ba7f42a796d5cced4dbde20d",
      "fb23e41eee404e1681c922bfda1f0c68",
      "1dbae508a6c44c8380c809c5acb13ddd",
      "d57001a6ba65463fb4e2bd4621f34f33",
      "d66f73a1b19540448d470371576bca83",
      "51c89c0f6e0e47d18921a58d801b2f63",
      "75d57c5cbf274d0ba193c3174436ba59",
      "9b8b3255b71343c9bdc4c9bbf1fec334",
      "23d54b49aeb8431f952e5a4684d306d9",
      "7e69d1332106437d85b83b5eddc77ec5",
      "8a02cb6a232f49179af3fa323fd7a454",
      "95d14bda117042398d9af86a756a3152",
      "bd63c7b0c5c040ba9541bd2fcef51758",
      "bad3cb7ac7994943a3a582b42278a1f8",
      "6dcc66d8f2a445c49350bf22b5ff0294",
      "4041adbf739847fd8e94d4b68c82955c",
      "0ec4eb7a45d045bda9ddab55c34b8a32",
      "eb3f54e018714fff9354e26b74f1323f",
      "e9a3b4516afb4d8a93cf2a273acf5250",
      "8b7bde7d326348ada0df55190cb19f40",
      "61fdf65c194e4a109b348f2b06868422",
      "6e8f4eef4c1240dc988e901039379cc9",
      "a3822e2c9a144b6ba3862242a7d45c11",
      "e31117d627db40b1a2614fb8d93ea9ff",
      "3ad70def5d9a4d4586fb2e68632c67d2",
      "679e128edd4d4c649ee0e652349539eb",
      "d95a076be82c4ddd9f73b003e5fb2cf3",
      "ab1d862212a0499fa73bdf7dd52113ec",
      "7235a60e23cb4f91aa79997515024da2",
      "884e184fd62d4d2bb599c42c12071d7c",
      "a4009f909cd14d45b383c54bdb0f8b1c",
      "7e4d5719ff144f7bba6ee64fd2da1705",
      "07d05bdabf13439e801368d8d71669f4",
      "1ab67407ab29460db2847ad7f406a6ae",
      "28827ff18f394b4f96afef0cb2657061",
      "5ce2477ccab74444b23e667e050f80d8",
      "0d7b0d9bab3841a385a985b631738310",
      "1b0cc9b414ec49acaa29a350537eb501",
      "11e55760c4bf4ca7bbbd6c76aed0a639",
      "605dec3cb9e34c6facf1e950c7502f0a",
      "246986c6101148c8ba5f5dbb5d005b27",
      "a49e73b3bdc6400da3903f982bc6d04b",
      "4e875ca7b1d6463a9cac747767578deb",
      "21f1ed46aaa141a5bb3b18ffa2c3ec8a",
      "8f85b000d9b047ccb087df6613a300aa",
      "1d74ff82049e414187206135c4d343fc",
      "2f1f7a3164b6495290f3713e7f16bad3",
      "d00413a5520942e9b3781157c8963241",
      "aee093ae1ca54185a96300136c291eca",
      "088b6089e6dd4d6981e6725e995b08e6",
      "a40370ec8cac44e9b504e76a0b4400ae",
      "62d80c02b0cc4f7a8d9132d7009b5956",
      "637687ffca774e9aa04b569cb783e868",
      "df3b12fff35a47f396c20810ad255b03",
      "716d1e2329654e2890512b5c3fee4b72",
      "600f6222fded403d8774aafda306df9d",
      "0e6effb2c0bb4b2684885a3fea92a4cf",
      "8f47ee7259e347d59e014380683efe82",
      "8579f1a93aae40358563cf973b94fdcb",
      "5eeeb0ad6ee840f58ea2788a61b23b12",
      "4914e6a968374755ae8f3f33adb0f234",
      "a59961195bb54dc9aff4361c02bc8de1",
      "d4d97096bd6740e29dc8eb433df5431c",
      "a9c2475224694ff3bfb80fbe57a1fb28",
      "4018af520be648a5a270a0d87bb16065",
      "20458297119b4d80bb0d8792b4e53cb3",
      "25385bed8021400f9719895ee82ab0dd",
      "2b43715603bd456182172b2fbf76cdd7",
      "52bb5e354ffd4c9187a6db8abb000b6a",
      "c0203eaa5aa3411386ca3fcd7928cb0e",
      "4b77514afd0a4c7793b2669080c4d536",
      "473123bbd1ab4876b12afb4949a95502",
      "5dadb931453647159eda4a23ccfb48ff",
      "3dc24387c3f04141a3eabc46c05182a0",
      "4752f3e2381d4cedaaaa84b5398fd8fc",
      "73ea01915aed40679bc760f21050398f",
      "45212dc9ec004f3386d619affb7a9fd3",
      "a125c58f4ded4590abb54d38f7e43f61",
      "cc1306eca0264b13a51fbe2cf7e4892e",
      "f158f5017b6e4d0eb810687dd261d674",
      "6b60df7692be4c3483739ffd44ea9330",
      "a615098947e44dea94da81b8e6e713d2",
      "406b57f23f7148f5aa613ad9bf6583e2",
      "4de9ab2c41e241c7a7f6727ab4a5da82",
      "ffd2871c1de341d1a06bd376cc984506",
      "04e18937beda476e9fb939e3fc3e908d",
      "5a00cc7519634a35911849774d063cfe",
      "9d5919e531dd41b2a39fde1d852f6315",
      "078cfe33751445ca9b4386f9d1c60985",
      "303291d220a948e2a61bcfed9b793b41",
      "5a94a608304c479a972772c72096abf0",
      "22f2b4423b2748b8ba2c5dbde91edb1d",
      "598ed931496c4cf4bcfcb036424aa54a",
      "c3e6a0769acb4f04942dbf041f6630d6",
      "978825cc0ef54d0da29e1d7984c42e1a",
      "72e1f1437f984212a3a24e28efb657ef",
      "90b65155158649888da232b24cb1cf2e",
      "1766dd5e4f7747d2af886ea4d8bf461b",
      "5c7eb0f86cde44feb6d76ab2dbff30cd",
      "b2605b2db94b4b929a9f7c927fdf88f5"
     ]
    },
    "id": "2-3ku6QR_ZWa",
    "outputId": "0d861fe0-e3f8-47e1-8a8b-cc7d1d061ff4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947a2ce0171c4582843c5afb590471fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb23e41eee404e1681c922bfda1f0c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd63c7b0c5c040ba9541bd2fcef51758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31117d627db40b1a2614fb8d93ea9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28827ff18f394b4f96afef0cb2657061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d74ff82049e414187206135c4d343fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6effb2c0bb4b2684885a3fea92a4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b43715603bd456182172b2fbf76cdd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1306eca0264b13a51fbe2cf7e4892e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303291d220a948e2a61bcfed9b793b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.6815312504768372\n",
      "Epoch 2/20, Loss: 0.5630680322647095\n",
      "Epoch 3/20, Loss: 0.41003280878067017\n",
      "Epoch 4/20, Loss: 0.32633399963378906\n",
      "Epoch 5/20, Loss: 0.22436682879924774\n",
      "Epoch 6/20, Loss: 0.09085053205490112\n",
      "Epoch 7/20, Loss: 0.04055032134056091\n",
      "Epoch 8/20, Loss: 0.013849971815943718\n",
      "Epoch 9/20, Loss: 0.008305830880999565\n",
      "Epoch 10/20, Loss: 0.003659198060631752\n",
      "Epoch 11/20, Loss: 0.0010380035964772105\n",
      "Epoch 12/20, Loss: 0.00029954835190437734\n",
      "Epoch 13/20, Loss: 0.00010289005876984447\n",
      "Epoch 14/20, Loss: 8.603346213931218e-05\n",
      "Epoch 15/20, Loss: 1.873927794804331e-05\n",
      "Epoch 16/20, Loss: 3.4570541629364016e-06\n",
      "Epoch 17/20, Loss: 1.0967236221404164e-06\n",
      "Epoch 18/20, Loss: 1.7404512391294702e-06\n",
      "Epoch 19/20, Loss: 5.412063728726935e-06\n",
      "Epoch 20/20, Loss: 7.152556946721234e-08\n",
      "Test Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Graph Embedding-based (Node2Vec)\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load necessary libraries for each method\n",
    "from node2vec import Node2Vec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create toy dataset with simple graphs\n",
    "num_graphs = 10\n",
    "graphs = [nx.erdos_renyi_graph(10, np.random.rand()) for _ in range(num_graphs)]\n",
    "\n",
    "# Labels for graph similarity task (could be used for graph classification or regression)\n",
    "labels = [np.random.choice([0,1]) for _ in range(num_graphs)]\n",
    "\n",
    "# Function to generate 2D histogram from node embeddings\n",
    "def generate_2d_histogram(node_embeddings, bins=16):\n",
    "    # Flatten embeddings to create histograms\n",
    "    embeddings = np.vstack(node_embeddings)\n",
    "    histogram, xedges, yedges = np.histogram2d(embeddings[:, 0], embeddings[:, 1], bins=bins)\n",
    "    return histogram\n",
    "\n",
    "# Prepare graph-level 2D histograms from node embeddings\n",
    "graph_histograms = []\n",
    "for i, graph in enumerate(graphs):\n",
    "    node2vec = Node2Vec(graph, dimensions=64, walk_length=10, num_walks=80, workers=4)\n",
    "    model = node2vec.fit()\n",
    "    node_embeddings = [model.wv.get_vector(str(node)) for node in graph.nodes()]\n",
    "    histogram = generate_2d_histogram(node_embeddings)\n",
    "    graph_histograms.append(histogram)\n",
    "\n",
    "# Convert histograms into tensors for CNN\n",
    "graph_histograms = np.array(graph_histograms)\n",
    "graph_histograms = torch.tensor(graph_histograms, dtype=torch.float32)\n",
    "\n",
    "# Split histograms into training and testing sets\n",
    "train_histograms, test_histograms, train_labels, test_labels = train_test_split(graph_histograms, labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define a simple 2D CNN model for graph classification\n",
    "class GraphCNN(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(GraphCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(1024, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(5, -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize and train the CNN model\n",
    "num_classes = 2  # Binary classification\n",
    "input_channels = 1  # Single channel for histogram\n",
    "bins = 16  # Same as used in generate_2d_histogram function\n",
    "\n",
    "train_histograms = train_histograms.unsqueeze(1)  # Add channel dimension\n",
    "test_histograms = test_histograms.unsqueeze(1)\n",
    "\n",
    "cnn_model = GraphCNN(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "cnn_model.train()\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = cnn_model(train_histograms)\n",
    "    loss = criterion(outputs, torch.tensor(train_labels, dtype=torch.long))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "cnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = cnn_model(test_histograms)\n",
    "    predictions = torch.argmax(test_outputs, axis=1)\n",
    "    accuracy = accuracy_score(test_labels, predictions.numpy())\n",
    "    print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpI9k7-N_url"
   },
   "source": [
    "## Graph kernel-based methods\n",
    "\n",
    "Graph kernel-based methods have generated a lot of interest in terms of capturing the similarity between graphs. These approaches compute the similarity between two graphs as a function of the similarities between some of their substructures. Different graph kernels exist based on the substructures they use, which include random walks, shortest paths, and subgraphs. As an example, a method called Deep Graph Kernels (DGK) (Yanardag et al., 2015) decomposes graphs into substructures that are viewed as \"words\". Then, natural language processing (NLP) approaches such as continuous bag of words (CBOW) and skip-gram are used to learn latent representations of the substructures. This way, the kernel between two graphs is defined based on the similarity of the substructure space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4beFzLP_c4c",
    "outputId": "89cfb937-f80d-4168-9ab2-06ebb4ebbcd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy using Deep Graph Kernels (DGK): 0.6\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create toy dataset with simple graphs\n",
    "num_graphs = 10\n",
    "graphs = [nx.erdos_renyi_graph(50, np.random.rand()) for _ in range(num_graphs)]\n",
    "\n",
    "# Labels for graph similarity task (could be used for graph classification or regression)\n",
    "labels = [np.random.choice([0,1]) for _ in range(num_graphs)]\n",
    "\n",
    "# Decompose graphs into substructures using random walks\n",
    "def random_walks_as_words(graph, walk_length=6, num_walks=10):\n",
    "    \"\"\"Generates 'words' from random walks on the graph.\"\"\"\n",
    "    walks = []\n",
    "    for _ in range(num_walks):\n",
    "        for node in graph.nodes():\n",
    "            walk = [str(node)]\n",
    "            for _ in range(walk_length - 1):\n",
    "                neighbors = list(graph.neighbors(int(walk[-1])))\n",
    "                if neighbors:\n",
    "                    walk.append(str(np.random.choice(neighbors)))\n",
    "                else:\n",
    "                    break\n",
    "            walks.append(\" \".join(walk))\n",
    "    return walks\n",
    "\n",
    "# Create a \"document\" for each graph using its random walks\n",
    "graph_documents = []\n",
    "for graph in graphs:\n",
    "    walks = random_walks_as_words(graph)\n",
    "    graph_documents.append(\" \".join(walks))  # Combine all walks into a single document\n",
    "\n",
    "# Use CountVectorizer to generate a \"bag of words\" representation for each graph\n",
    "vectorizer = CountVectorizer()\n",
    "graph_bow = vectorizer.fit_transform(graph_documents)  # Sparse matrix of shape (num_graphs, num_features)\n",
    "\n",
    "# Compute pairwise similarities between graphs using cosine similarity\n",
    "graph_similarity_matrix = cosine_similarity(graph_bow)\n",
    "\n",
    "# Use the similarity matrix as input features for a classification task\n",
    "# Split dataset into training and testing sets\n",
    "train_indices, test_indices = train_test_split(range(len(graphs)), test_size=0.5, random_state=42)\n",
    "\n",
    "train_similarity = graph_similarity_matrix[np.ix_(train_indices, train_indices)]\n",
    "test_similarity = graph_similarity_matrix[np.ix_(test_indices, train_indices)]  # Test against training similarities\n",
    "\n",
    "train_labels = [labels[i] for i in train_indices]\n",
    "test_labels = [labels[i] for i in test_indices]\n",
    "\n",
    "# Train a classifier using the training similarity matrix\n",
    "svm = SVC(kernel=\"precomputed\")  # Precomputed kernel\n",
    "svm.fit(train_similarity, train_labels)\n",
    "\n",
    "# Predict on the test set\n",
    "test_predictions = svm.predict(test_similarity)\n",
    "\n",
    "# Evaluate the classification accuracy\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy using Deep Graph Kernels (DGK): {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uY5Ysjt-_6eu"
   },
   "source": [
    "## GNN-based methods\n",
    "\n",
    "With the emergence of deep learning (DL) techniques, GNNs have become a powerful new tool for learning representations on graphs. Such powerful models can be easily adapted to various tasks, including graph similarity learning. Furthermore, they present a key advantage with respect to other traditional graph embedding approaches. Indeed, while the latter generally learn the representation in an isolated stage, in this kind of approach, the representation learning and the target learning task are conducted jointly. Therefore, the GNN deep models can better leverage the graph features for the specific learning task. We have already seen an example of similarity learning using GNNs in Chapter 4, Unsupervised Graph Learning, where a two-branch network was trained to estimate the proximity distance between two graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PBnQ_Du_gMu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Create toy dataset with simple graphs\n",
    "num_graphs = 10\n",
    "graphs = [nx.erdos_renyi_graph(10, np.random.rand()) for _ in range(num_graphs)]\n",
    "\n",
    "# Generate similarity labels (e.g., based on structural similarity)\n",
    "labels = [np.random.rand() for _ in range(num_graphs)]  # Similarity score between 0 and 1\n",
    "\n",
    "# Convert graphs to PyTorch Geometric Data objects\n",
    "def nx_to_data(graph, label):\n",
    "    edge_index = torch.tensor(list(graph.edges), dtype=torch.long).t().contiguous()\n",
    "    x = torch.eye(graph.number_of_nodes(), dtype=torch.float)  # Node features as identity matrix\n",
    "    return Data(x=x, edge_index=edge_index, y=torch.tensor([label], dtype=torch.float))\n",
    "\n",
    "data_list = [nx_to_data(graph, labels[i]) for i, graph in enumerate(graphs)]\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "train_data, test_data = train_test_split(data_list, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(train_data, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=2, shuffle=False)\n",
    "\n",
    "# Define a GNN model for graph-level representation\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        x = self.relu(self.conv2(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)  # Graph-level pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Define a two-branch similarity learning network\n",
    "class SimilarityNet(nn.Module):\n",
    "    def __init__(self, gnn, embedding_dim):\n",
    "        super(SimilarityNet, self).__init__()\n",
    "        self.gnn = gnn\n",
    "        self.fc = nn.Linear(2 * embedding_dim, 1)  # Combine embeddings from two branches\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        # Extract graph-level embeddings for both graphs\n",
    "        x1 = self.gnn(data1.x, data1.edge_index, data1.batch)\n",
    "        x2 = self.gnn(data2.x, data2.edge_index, data2.batch)\n",
    "        # Concatenate embeddings and pass through a fully connected layer\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        similarity_score = self.fc(x)\n",
    "        return similarity_score\n",
    "\n",
    "# Initialize models and hyperparameters\n",
    "input_dim = 10  # Number of node features\n",
    "hidden_dim = 32\n",
    "embedding_dim = 16\n",
    "gnn = GNN(input_dim, hidden_dim, embedding_dim)\n",
    "model = SimilarityNet(gnn, embedding_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for data1, data2 in zip(train_loader, train_loader):  # Pair graphs for similarity learning\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(data1, data2)\n",
    "        target = (data1.y + data2.y) / 2  # Example: Average similarity score\n",
    "        loss = criterion(pred.view(-1), target.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "predictions, targets = [], []\n",
    "with torch.no_grad():\n",
    "    for data1, data2 in zip(test_loader, test_loader):\n",
    "        pred = model(data1, data2)\n",
    "        target = (data1.y + data2.y) / 2\n",
    "        predictions.extend(pred.view(-1).tolist())\n",
    "        targets.extend(target.view(-1).tolist())\n",
    "\n",
    "mse = mean_squared_error(targets, predictions)\n",
    "print(f\"Test Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCfFB-iv4Ihd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "chap6",
   "language": "python",
   "name": "chap6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
